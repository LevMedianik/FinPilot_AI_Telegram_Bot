import os
import fitz
import docx
from uuid import uuid4
from dotenv import load_dotenv
import requests
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document

load_dotenv()

# ----------------------------
# Storage (files + FAISS index)
# ----------------------------
DATA_DIR = "./data"
INDEX_DIR = "./faiss_index"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(INDEX_DIR, exist_ok=True)

def save_file(file_bytes, original_filename: str) -> str:
    file_id = str(uuid4())
    filename = f"{file_id}_{original_filename}"
    filepath = os.path.join(DATA_DIR, filename)
    with open(filepath, "wb") as f:
        f.write(file_bytes)
    return filepath

# ----------------------------
# Text extraction
# ----------------------------
def extract_text_from_pdf(filepath: str) -> str:
    doc = fitz.open(filepath)
    return "".join(page.get_text() for page in doc)

def extract_text_from_docx(filepath: str) -> str:
    doc = docx.Document(filepath)
    return "\n".join(para.text for para in doc.paragraphs)

def extract_text_from_txt(filepath: str) -> str:
    with open(filepath, "r", encoding="utf-8") as f:
        return f.read()

def extract_text_from_file(filepath: str) -> str:
    lower = filepath.lower()
    if lower.endswith(".pdf"):
        return extract_text_from_pdf(filepath)
    if lower.endswith(".docx"):
        return extract_text_from_docx(filepath)
    if lower.endswith(".txt"):
        return extract_text_from_txt(filepath)
    raise ValueError("‚ùå –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF, DOCX –∏ TXT —Ñ–∞–π–ª—ã.")

# ----------------------------
# Vector index (FAISS)
# ----------------------------
_EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")

def index_text_with_faiss(text: str):
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    chunks = splitter.split_text(text)
    documents = [Document(page_content=chunk) for chunk in chunks]

    embeddings = HuggingFaceEmbeddings(model_name=_EMBED_MODEL)
    vectorstore = FAISS.from_documents(documents, embedding=embeddings)
    vectorstore.save_local(INDEX_DIR)
    return vectorstore

def load_existing_index():
    index_file = os.path.join(INDEX_DIR, "index.faiss")
    if not os.path.exists(index_file):
        return None
    embeddings = HuggingFaceEmbeddings(model_name=_EMBED_MODEL)
    return FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)

# ----------------------------
# OpenRouter LLM (robust routing)
# ----------------------------
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

DEFAULT_MODELS = [
    "deepseek/deepseek-chat-v3-0324",  # stable id (may require credits)
    "deepseek/deepseek-chat",          # alias / fallback
]

def _models_from_env():
    raw = os.getenv("OPENROUTER_MODELS", "").strip()
    if not raw:
        return DEFAULT_MODELS
    return [m.strip() for m in raw.split(",") if m.strip()]

OPENROUTER_MODELS = _models_from_env()

HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}" if OPENROUTER_API_KEY else "",
    "Content-Type": "application/json",
    "HTTP-Referer": os.getenv("OPENROUTER_REFERER", "http://localhost"),
    "X-Title": os.getenv("OPENROUTER_APP_NAME", "BizSense"),
}

OPENROUTER_URL = os.getenv("OPENROUTER_URL", "https://openrouter.ai/api/v1/chat/completions")

def call_openrouter(messages, temperature: float = 0.3, max_tokens: int | None = None) -> str:
    """
    Tries models in OPENROUTER_MODELS until one responds successfully.
    Returns assistant text or a readable error message.
    """
    if not OPENROUTER_API_KEY:
        return "–û—à–∏–±–∫–∞: –Ω–µ –∑–∞–¥–∞–Ω OPENROUTER_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è."

    last_err = None
    for model in OPENROUTER_MODELS:
        body = {"model": model, "messages": messages, "temperature": temperature}
        if max_tokens is not None:
            body["max_tokens"] = max_tokens

        try:
            resp = requests.post(OPENROUTER_URL, headers=HEADERS, json=body, timeout=60)
        except Exception as e:
            last_err = f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM: {e}"
            continue

        if resp.status_code == 200:
            data = resp.json()
            try:
                return data["choices"][0]["message"]["content"]
            except Exception:
                return f"–û—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ LLM: {data}"

        try:
            err_json = resp.json()
        except Exception:
            err_json = {"raw": resp.text}

        msg = err_json.get("error", {}).get("message") or str(err_json)
        last_err = f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM ({resp.status_code}) –¥–ª—è –º–æ–¥–µ–ª–∏ {model}: {msg}"

        if resp.status_code == 404 and "No endpoints found" in msg:
            continue  # try next model

        break

    return last_err or "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM."

DEFAULT_SYSTEM_PROMPT = os.getenv(
    "SYSTEM_PROMPT",
    "–¢—ã ‚Äì –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. "
    "–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç–∞–º–∏ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
    "–ù–ï –¥–æ–±–∞–≤–ª—è–π –≤—ã–≤–æ–¥—ã, –æ–±–æ–±—â–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏. "
    "–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã '–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞', '–º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å', '–≤–µ—Ä–æ—è—Ç–Ω–æ', '—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ'. "
    "–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ ‚Äì —Å–∫–∞–∂–∏: '–í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —ç—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'. "
    "–§–æ—Ä–º–∞—Ç: 2‚Äì6 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—É–Ω–∫—Ç–æ–≤ –∏–ª–∏ 2‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ë–µ–∑ –ª–∏—à–Ω–µ–≥–æ."
)

# --- RAG gating (anti-hallucination) ---
RAG_MIN_CONTEXT_CHARS = int(os.getenv("RAG_MIN_CONTEXT_CHARS", "300"))
RAG_MAX_L2_DISTANCE = float(os.getenv("RAG_MAX_L2_DISTANCE", "1.1"))
RAG_REFUSAL_TEXT = os.getenv(
    "RAG_REFUSAL_TEXT",
    "–í —Ç–µ–∫—É—â–µ–π –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.\n"
    "–Ø –æ—Ç–≤–µ—á–∞—é —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. "
    "–£—Ç–æ—á–Ω–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥—Ä—É–≥–æ–π –¥–æ–∫—É–º–µ–Ω—Ç."
)

def _normalize_ru(s: str) -> str:
    s = s.lower()
    for ch in ",.;:!?()[]{}\"'¬´¬ª‚Äì‚Äì-":
        s = s.replace(ch, " ")
    return " ".join(s.split())

def _has_overlap(question: str, context: str, min_hits: int = 1) -> bool:
    q = _normalize_ru(question)
    c = _normalize_ru(context)

    # –±–∞–∑–æ–≤—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ)
    stop = {"—á—Ç–æ", "–∫–∞–∫", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–¥–ª—è", "–≤", "–Ω–∞", "–∏", "–∏–ª–∏", "—ç—Ç–æ", "—Ç–∞–∫–∏–µ"}
    q_words = [w for w in q.split() if len(w) >= 5 and w not in stop]

    hits = sum(1 for w in set(q_words) if w in c)
    return hits >= min_hits

def _build_strict_rag_prompt(context: str, question: str) -> str:
    return (
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç (–≤—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):\n"
        f"{context}\n"
        "---\n"
        f"–í–æ–ø—Ä–æ—Å: {question}\n\n"
        "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û —Ç–µ–º, —á—Ç–æ —è–≤–Ω–æ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.\n"
        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (—Å—Ç—Ä–æ–≥–æ):\n"
        "–ü—É–Ω–∫—Ç: <–∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç>. –¶–∏—Ç–∞—Ç–∞: \"<–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞>\"\n"
        "–ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å –¥–∞—Ç—å —Ü–∏—Ç–∞—Ç—É ‚Äì –æ—Ç–≤–µ—Ç—å: \"–í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —ç—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\"\n"
        "–û—Ç–≤–µ—Ç:"
    )

def query_index(question: str, announce: bool = False):
    vectorstore = load_existing_index()
    if not vectorstore:
        return "‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç."

    # 1) –î–æ—Å—Ç–∞—ë–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–µ —Å–æ score (distance)
    # –î–ª—è FAISS –≤ LangChain –æ–±—ã—á–Ω–æ —ç—Ç–æ L2-distance: —á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ.
    try:
        hits = vectorstore.similarity_search_with_score(question, k=8)
    except Exception:
        # fallback, –µ—Å–ª–∏ —É —Ç–≤–æ–µ–π –≤–µ—Ä—Å–∏–∏ –Ω–µ—Ç with_score
        retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 4})
        results = retriever.get_relevant_documents(question)
        context = "\n".join(doc.page_content.strip() for doc in results).strip()
        if len(context) < RAG_MIN_CONTEXT_CHARS:
            return (("üîç –ò—â—É –æ—Ç–≤–µ—Ç...", RAG_REFUSAL_TEXT) if announce else RAG_REFUSAL_TEXT)
        
        # –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å—Ç—å ‚Äì –∏–¥—ë–º –≤ LLM
        full_prompt = _build_strict_rag_prompt(context, question)
        messages = [
            {"role": "system", "content": DEFAULT_SYSTEM_PROMPT},
            {"role": "user", "content": full_prompt},
        ]
        reply = call_openrouter(messages=messages, temperature=0.3)
        return ("üîç –ò—â—É –æ—Ç–≤–µ—Ç...", reply) if announce else reply


    # 2) –°–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ + –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    if not hits:
        return (("üîç –ò—â—É –æ—Ç–≤–µ—Ç...", RAG_REFUSAL_TEXT) if announce else RAG_REFUSAL_TEXT)

    docs = [doc for (doc, _score) in hits]
    scores = [_score for (_doc, _score) in hits]

    context = "\n".join(d.page_content.strip() for d in docs).strip()
    best_score = min(scores) if scores else 999.0

    # Gate A: –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–æ–±—ã—á–Ω–æ –æ–∑–Ω–∞—á–∞–µ—Ç "–Ω–µ –Ω–∞—à–ª–æ—Å—å")
    # Gate B: –¥–∞–∂–µ –ª—É—á—à–∏–π score —Å–ª–∞–±—ã–π (–¥–∞–ª–µ–∫–æ –æ—Ç –≤–æ–ø—Ä–æ—Å–∞)
    if len(context) < RAG_MIN_CONTEXT_CHARS or best_score > RAG_MAX_L2_DISTANCE:
        return (("üîç –ò—â—É –æ—Ç–≤–µ—Ç...", RAG_REFUSAL_TEXT) if announce else RAG_REFUSAL_TEXT)
    min_hits = 0 if len(question.strip()) < 35 else 1
    if not _has_overlap(question, context, min_hits=min_hits):
        return (("üîç –ò—â—É –æ—Ç–≤–µ—Ç...", RAG_REFUSAL_TEXT) if announce else RAG_REFUSAL_TEXT)

    # 3) –ï—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äì –∑–æ–≤—ë–º LLM
    full_prompt = _build_strict_rag_prompt(context, question)
    messages = [
        {"role": "system", "content": DEFAULT_SYSTEM_PROMPT},
        {"role": "user", "content": full_prompt},
    ]
    
    reply = call_openrouter(messages=messages, temperature=0.3)
    return ("üîç –ò—â—É –æ—Ç–≤–µ—Ç...", reply) if announce else reply

def summarize_pdf(announce: bool = False):
    vectorstore = load_existing_index()
    if not vectorstore:
        return "‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç."

    retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 6})
    results = retriever.get_relevant_documents("–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Ç–µ–∑–∏—Å—ã, –≤—ã–≤–æ–¥—ã")

    text = "\n".join(doc.page_content.strip() for doc in results)
    prompt = (
        "–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ 7‚Äì12 –ø—É–Ω–∫—Ç–æ–≤. "
        "–£–∫–∞–∂–∏ –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥—ã.\n\n"
        f"{text}"
    )

    messages = [
        {"role": "system", "content": DEFAULT_SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
    ]
    reply = call_openrouter(messages=messages, temperature=0.3)
    return ("üìñ –ü–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞—é —Ç–µ–∫—Å—Ç...", reply) if announce else reply
